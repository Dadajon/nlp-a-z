{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Seq2Seq.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "oW1LUhau69um",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **<center>Seq2Seq Colab Training</center>**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "![https://google.github.io/seq2seq/t](https://3.bp.blogspot.com/-3Pbj_dvt0Vo/V-qe-Nl6P5I/AAAAAAAABQc/z0_6WtVWtvARtMk0i9_AtLeyyGyV6AI4wCLcB/s1600/nmt-model-fast.gif)\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "##**Welcome to our setup for the Google Colab notebook with GPU to help train the Seq2Seq Chatbot Model. Credit for this setup goes to fellow student Onur Yartasi, who built the original Colab notebook and setup that can be found here with the original steps: https://gist.github.com/onuryartasi/7b861ff3cff77bcf68846db3bec0b2a6.**\n",
        "##**This is a slightly modified version mainly related to the CUDA setup.**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "oyy_XosLTdOn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Google Colab is using the latest Tensorflow version when you launch a new notebook at default. We need to use v1.0.0 for the first model in the course. We can install it with the following command: **\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "0jCLPrSMMo8R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip3 install tensorflow==1.0.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mDZhxZ1BTqsD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**[Now we can remove the default cuda version because TF 1.0.0 needs 8.0**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "9RY_NG_LMu91",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "!apt-get remove cuda\n",
        "!apt-get autoremove cuda\n",
        "!apt-get purge cuda\n",
        "!apt-key del /var/cuda-repo-9-2-local/*.pub\n",
        "!rm -rf /var/cuda-repo-8-0-local-ga2/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6pyLthipT7LI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Now you can set up CUDA with 2 repo options. You can follow the steps at https://gist.github.com/onuryartasi/7b861ff3cff77bcf68846db3bec0b2a6 and authenticate your Google Drive. **\n",
        "\n",
        "\n",
        "**Or you can use the following to retrieve the repo for the .deb file. This setup will fail during the install so if you do go this route please use the following steps in a row. More information can be found from https://medium.com/@nickzamosenchuk/training-the-model-for-ios-coreml-in-google-colab-60-times-faster-6b3d1669fc46 **\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "0eGGSyc3Of_H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/cuda-repo-ubuntu1604_8.0.61-1_amd64.deb\n",
        "!dpkg -i --force-overwrite cuda-repo-ubuntu1604_8.0.61-1_amd64.deb\n",
        "!apt-get update\n",
        "!apt-get install cuda-8-0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eLitZlnePS8K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget http://archive.ubuntu.com/ubuntu/pool/main/m/mesa/libglx-mesa0_18.0.5-0ubuntu0~18.04.1_amd64.deb\n",
        "!dpkg -i --force-overwrite libglx-mesa0_18.0.5-0ubuntu0~18.04.1_amd64.deb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d0XaaSVuPUAu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/nvidia-410_410.48-0ubuntu1_amd64.deb\n",
        "!dpkg -i --force-overwrite nvidia-410_410.48-0ubuntu1_amd64.deb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xynjif1zPalI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!apt --fix-broken install\n",
        "!apt-get install cuda-8-0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Obd-6BC4R8Yn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " **This will have CUDA 8.0 installed. We can check the version now:**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "KB3u_d7bPiGT",
        "colab_type": "code",
        "outputId": "5cff8949-f892-4a31-decd-dfa28ee99ec4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2016 NVIDIA Corporation\n",
            "Built on Tue_Jan_10_13:22:03_CST_2017\n",
            "Cuda compilation tools, release 8.0, V8.0.61\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Um1_fobRUcy8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Install Tensorflow GPU for GPU usage (double check that the runtime environment is set for GPU)**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "zXT0NawWOmZR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-gpu==1.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2g25MGlLUlFs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Check the GPU and version**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "dsJ9PcGmQHgi",
        "colab_type": "code",
        "outputId": "273d3d72-2a01-495d-b05f-b020ad631ca4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/gpu:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "metadata": {
        "id": "v1b8IR0DUszo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Version:**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "tV5kYQMuQLYx",
        "colab_type": "code",
        "outputId": "b65cfc2a-9f95-4276-e510-5e6e3053f48a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "tf.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.0.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "id": "YDI5DBKvUuoz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Import the files that we need for our project, if you have them downloaded locally you can use the following to upload them:**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "nEuvP18YQNio",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p4lu-CsGU6al",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**We can now run the python file from our uploaded files with: **\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "6JoKZ9kFQrK3",
        "colab_type": "code",
        "outputId": "bdcb9325-d6cc-4ab6-bb92-c1c3de9a5145",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3617
        }
      },
      "cell_type": "code",
      "source": [
        "!python chatbot.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\n",
            "I tensorflow/stream_executor/dso_loader.cc:126] Couldn't open CUDA library libcudnn.so.5. LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "I tensorflow/stream_executor/cuda/cuda_dnn.cc:3517] Unable to load cuDNN DSO\n",
            "I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\n",
            "I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\n",
            "I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\n",
            "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\n",
            "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n",
            "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n",
            "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n",
            "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n",
            "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n",
            "I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:910] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: \n",
            "name: Tesla K80\n",
            "major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n",
            "pciBusID 0000:00:04.0\n",
            "Total memory: 11.17GiB\n",
            "Free memory: 11.10GiB\n",
            "I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 \n",
            "I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y \n",
            "I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0)\n",
            "Epoch:   1/100, Batch:    0/4120, Training Loss Error:  0.089, Training Time on 100 Batches: 101 seconds\n",
            "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 5733 get requests, put_count=4793 evicted_count=1000 eviction_rate=0.208638 and unsatisfied allocation rate=0.355835\n",
            "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110\n",
            "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 3249 get requests, put_count=3412 evicted_count=1000 eviction_rate=0.293083 and unsatisfied allocation rate=0.264081\n",
            "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 233 to 256\n",
            "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 6271 get requests, put_count=6397 evicted_count=1000 eviction_rate=0.156323 and unsatisfied allocation rate=0.147983\n",
            "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 596 to 655\n",
            "Epoch:   1/100, Batch:  100/4120, Training Loss Error:  2.967, Training Time on 100 Batches: 49 seconds\n",
            "Epoch:   1/100, Batch:  200/4120, Training Loss Error:  2.359, Training Time on 100 Batches: 38 seconds\n",
            "Epoch:   1/100, Batch:  300/4120, Training Loss Error:  2.284, Training Time on 100 Batches: 47 seconds\n",
            "Epoch:   1/100, Batch:  400/4120, Training Loss Error:  2.215, Training Time on 100 Batches: 45 seconds\n",
            "Epoch:   1/100, Batch:  500/4120, Training Loss Error:  2.227, Training Time on 100 Batches: 47 seconds\n",
            "Epoch:   1/100, Batch:  600/4120, Training Loss Error:  2.192, Training Time on 100 Batches: 44 seconds\n",
            "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 1694334 get requests, put_count=1694316 evicted_count=3000 eviction_rate=0.00177063 and unsatisfied allocation rate=0.0018491\n",
            "Epoch:   1/100, Batch:  700/4120, Training Loss Error:  2.156, Training Time on 100 Batches: 49 seconds\n",
            "Epoch:   1/100, Batch:  800/4120, Training Loss Error:  2.173, Training Time on 100 Batches: 48 seconds\n",
            "Epoch:   1/100, Batch:  900/4120, Training Loss Error:  2.185, Training Time on 100 Batches: 49 seconds\n",
            "Epoch:   1/100, Batch: 1000/4120, Training Loss Error:  2.152, Training Time on 100 Batches: 47 seconds\n",
            "Epoch:   1/100, Batch: 1100/4120, Training Loss Error:  2.155, Training Time on 100 Batches: 51 seconds\n",
            "Epoch:   1/100, Batch: 1200/4120, Training Loss Error:  2.102, Training Time on 100 Batches: 50 seconds\n",
            "Epoch:   1/100, Batch: 1300/4120, Training Loss Error:  2.122, Training Time on 100 Batches: 51 seconds\n",
            "Epoch:   1/100, Batch: 1400/4120, Training Loss Error:  2.042, Training Time on 100 Batches: 50 seconds\n",
            "Epoch:   1/100, Batch: 1500/4120, Training Loss Error:  2.067, Training Time on 100 Batches: 49 seconds\n",
            "Epoch:   1/100, Batch: 1600/4120, Training Loss Error:  2.054, Training Time on 100 Batches: 48 seconds\n",
            "Epoch:   1/100, Batch: 1700/4120, Training Loss Error:  2.094, Training Time on 100 Batches: 52 seconds\n",
            "Epoch:   1/100, Batch: 1800/4120, Training Loss Error:  2.058, Training Time on 100 Batches: 45 seconds\n",
            "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 4276849 get requests, put_count=4276898 evicted_count=9000 eviction_rate=0.00210433 and unsatisfied allocation rate=0.00212259\n",
            "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 1400 to 1540\n",
            "Epoch:   1/100, Batch: 1900/4120, Training Loss Error:  2.038, Training Time on 100 Batches: 53 seconds\n",
            "Epoch:   1/100, Batch: 2000/4120, Training Loss Error:  2.036, Training Time on 100 Batches: 53 seconds\n",
            "Validation Loss Error:  1.991, Batch Validation Time: 89 seconds\n",
            "I speak better now!!\n",
            "Epoch:   1/100, Batch: 2100/4120, Training Loss Error:  2.015, Training Time on 100 Batches: 49 seconds\n",
            "Epoch:   1/100, Batch: 2200/4120, Training Loss Error:  2.124, Training Time on 100 Batches: 48 seconds\n",
            "Epoch:   1/100, Batch: 2300/4120, Training Loss Error:  2.045, Training Time on 100 Batches: 53 seconds\n",
            "Epoch:   1/100, Batch: 2400/4120, Training Loss Error:  2.011, Training Time on 100 Batches: 53 seconds\n",
            "Epoch:   1/100, Batch: 2500/4120, Training Loss Error:  2.071, Training Time on 100 Batches: 47 seconds\n",
            "Epoch:   1/100, Batch: 2600/4120, Training Loss Error:  2.035, Training Time on 100 Batches: 51 seconds\n",
            "Epoch:   1/100, Batch: 2700/4120, Training Loss Error:  2.061, Training Time on 100 Batches: 56 seconds\n",
            "Epoch:   1/100, Batch: 2800/4120, Training Loss Error:  2.051, Training Time on 100 Batches: 55 seconds\n",
            "Epoch:   1/100, Batch: 2900/4120, Training Loss Error:  2.031, Training Time on 100 Batches: 56 seconds\n",
            "Epoch:   1/100, Batch: 3000/4120, Training Loss Error:  2.001, Training Time on 100 Batches: 51 seconds\n",
            "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 1938594 get requests, put_count=1938557 evicted_count=4000 eviction_rate=0.00206339 and unsatisfied allocation rate=0.00216188\n",
            "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 1694 to 1863\n",
            "Epoch:   1/100, Batch: 3100/4120, Training Loss Error:  2.090, Training Time on 100 Batches: 50 seconds\n",
            "Epoch:   1/100, Batch: 3200/4120, Training Loss Error:  1.962, Training Time on 100 Batches: 53 seconds\n",
            "Epoch:   1/100, Batch: 3300/4120, Training Loss Error:  1.983, Training Time on 100 Batches: 59 seconds\n",
            "Epoch:   1/100, Batch: 3400/4120, Training Loss Error:  2.020, Training Time on 100 Batches: 59 seconds\n",
            "Epoch:   1/100, Batch: 3500/4120, Training Loss Error:  1.978, Training Time on 100 Batches: 58 seconds\n",
            "Epoch:   1/100, Batch: 3600/4120, Training Loss Error:  1.976, Training Time on 100 Batches: 59 seconds\n",
            "Epoch:   1/100, Batch: 3700/4120, Training Loss Error:  2.007, Training Time on 100 Batches: 55 seconds\n",
            "Epoch:   1/100, Batch: 3800/4120, Training Loss Error:  1.951, Training Time on 100 Batches: 64 seconds\n",
            "Epoch:   1/100, Batch: 3900/4120, Training Loss Error:  2.011, Training Time on 100 Batches: 61 seconds\n",
            "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 1152456 get requests, put_count=1152466 evicted_count=3000 eviction_rate=0.00260311 and unsatisfied allocation rate=0.00277147\n",
            "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 2253 to 2478\n",
            "Epoch:   1/100, Batch: 4000/4120, Training Loss Error:  1.993, Training Time on 100 Batches: 62 seconds\n",
            "Epoch:   1/100, Batch: 4100/4120, Training Loss Error:  1.996, Training Time on 100 Batches: 66 seconds\n",
            "Validation Loss Error:  1.946, Batch Validation Time: 89 seconds\n",
            "I speak better now!!\n",
            "Epoch:   2/100, Batch:    0/4120, Training Loss Error:  0.384, Training Time on 100 Batches: 43 seconds\n",
            "Epoch:   2/100, Batch:  100/4120, Training Loss Error:  1.937, Training Time on 100 Batches: 47 seconds\n",
            "Epoch:   2/100, Batch:  200/4120, Training Loss Error:  1.938, Training Time on 100 Batches: 38 seconds\n",
            "Epoch:   2/100, Batch:  300/4120, Training Loss Error:  1.913, Training Time on 100 Batches: 47 seconds\n",
            "Epoch:   2/100, Batch:  400/4120, Training Loss Error:  1.869, Training Time on 100 Batches: 45 seconds\n",
            "Epoch:   2/100, Batch:  500/4120, Training Loss Error:  1.913, Training Time on 100 Batches: 47 seconds\n",
            "Epoch:   2/100, Batch:  600/4120, Training Loss Error:  1.893, Training Time on 100 Batches: 44 seconds\n",
            "Epoch:   2/100, Batch:  700/4120, Training Loss Error:  1.876, Training Time on 100 Batches: 49 seconds\n",
            "Epoch:   2/100, Batch:  800/4120, Training Loss Error:  1.904, Training Time on 100 Batches: 47 seconds\n",
            "Epoch:   2/100, Batch:  900/4120, Training Loss Error:  1.931, Training Time on 100 Batches: 48 seconds\n",
            "Epoch:   2/100, Batch: 1000/4120, Training Loss Error:  1.913, Training Time on 100 Batches: 47 seconds\n",
            "Epoch:   2/100, Batch: 1100/4120, Training Loss Error:  1.933, Training Time on 100 Batches: 51 seconds\n",
            "Epoch:   2/100, Batch: 1200/4120, Training Loss Error:  1.898, Training Time on 100 Batches: 49 seconds\n",
            "Epoch:   2/100, Batch: 1300/4120, Training Loss Error:  1.925, Training Time on 100 Batches: 51 seconds\n",
            "Epoch:   2/100, Batch: 1400/4120, Training Loss Error:  1.865, Training Time on 100 Batches: 52 seconds\n",
            "Epoch:   2/100, Batch: 1500/4120, Training Loss Error:  1.900, Training Time on 100 Batches: 49 seconds\n",
            "Epoch:   2/100, Batch: 1600/4120, Training Loss Error:  1.887, Training Time on 100 Batches: 47 seconds\n",
            "Epoch:   2/100, Batch: 1700/4120, Training Loss Error:  1.925, Training Time on 100 Batches: 52 seconds\n",
            "Epoch:   2/100, Batch: 1800/4120, Training Loss Error:  1.903, Training Time on 100 Batches: 44 seconds\n",
            "Epoch:   2/100, Batch: 1900/4120, Training Loss Error:  1.884, Training Time on 100 Batches: 52 seconds\n",
            "Epoch:   2/100, Batch: 2000/4120, Training Loss Error:  1.888, Training Time on 100 Batches: 52 seconds\n",
            "Validation Loss Error:  1.857, Batch Validation Time: 89 seconds\n",
            "I speak better now!!\n",
            "Epoch:   2/100, Batch: 2100/4120, Training Loss Error:  1.871, Training Time on 100 Batches: 48 seconds\n",
            "Epoch:   2/100, Batch: 2200/4120, Training Loss Error:  1.981, Training Time on 100 Batches: 47 seconds\n",
            "Epoch:   2/100, Batch: 2300/4120, Training Loss Error:  1.910, Training Time on 100 Batches: 54 seconds\n",
            "Epoch:   2/100, Batch: 2400/4120, Training Loss Error:  1.883, Training Time on 100 Batches: 55 seconds\n",
            "Epoch:   2/100, Batch: 2500/4120, Training Loss Error:  1.938, Training Time on 100 Batches: 48 seconds\n",
            "Epoch:   2/100, Batch: 2600/4120, Training Loss Error:  1.916, Training Time on 100 Batches: 52 seconds\n",
            "Epoch:   2/100, Batch: 2700/4120, Training Loss Error:  1.935, Training Time on 100 Batches: 57 seconds\n",
            "Epoch:   2/100, Batch: 2800/4120, Training Loss Error:  1.930, Training Time on 100 Batches: 54 seconds\n",
            "Epoch:   2/100, Batch: 2900/4120, Training Loss Error:  1.917, Training Time on 100 Batches: 56 seconds\n",
            "Epoch:   2/100, Batch: 3000/4120, Training Loss Error:  1.894, Training Time on 100 Batches: 53 seconds\n",
            "Epoch:   2/100, Batch: 3100/4120, Training Loss Error:  1.982, Training Time on 100 Batches: 50 seconds\n",
            "Epoch:   2/100, Batch: 3200/4120, Training Loss Error:  1.855, Training Time on 100 Batches: 53 seconds\n",
            "Epoch:   2/100, Batch: 3300/4120, Training Loss Error:  1.883, Training Time on 100 Batches: 58 seconds\n",
            "Epoch:   2/100, Batch: 3400/4120, Training Loss Error:  1.918, Training Time on 100 Batches: 59 seconds\n",
            "Epoch:   2/100, Batch: 3500/4120, Training Loss Error:  1.881, Training Time on 100 Batches: 60 seconds\n",
            "Epoch:   2/100, Batch: 3600/4120, Training Loss Error:  1.879, Training Time on 100 Batches: 58 seconds\n",
            "Epoch:   2/100, Batch: 3700/4120, Training Loss Error:  1.913, Training Time on 100 Batches: 55 seconds\n",
            "Epoch:   2/100, Batch: 3800/4120, Training Loss Error:  1.860, Training Time on 100 Batches: 65 seconds\n",
            "Epoch:   2/100, Batch: 3900/4120, Training Loss Error:  1.916, Training Time on 100 Batches: 59 seconds\n",
            "Epoch:   2/100, Batch: 4000/4120, Training Loss Error:  1.902, Training Time on 100 Batches: 62 seconds\n",
            "Epoch:   2/100, Batch: 4100/4120, Training Loss Error:  1.908, Training Time on 100 Batches: 66 seconds\n",
            "Validation Loss Error:  1.831, Batch Validation Time: 89 seconds\n",
            "I speak better now!!\n",
            "Epoch:   3/100, Batch:    0/4120, Training Loss Error:  0.368, Training Time on 100 Batches: 41 seconds\n",
            "Epoch:   3/100, Batch:  100/4120, Training Loss Error:  1.856, Training Time on 100 Batches: 47 seconds\n",
            "Epoch:   3/100, Batch:  200/4120, Training Loss Error:  1.858, Training Time on 100 Batches: 38 seconds\n",
            "Epoch:   3/100, Batch:  300/4120, Training Loss Error:  1.838, Training Time on 100 Batches: 47 seconds\n",
            "Epoch:   3/100, Batch:  400/4120, Training Loss Error:  1.796, Training Time on 100 Batches: 44 seconds\n",
            "Epoch:   3/100, Batch:  500/4120, Training Loss Error:  1.841, Training Time on 100 Batches: 47 seconds\n",
            "Epoch:   3/100, Batch:  600/4120, Training Loss Error:  1.822, Training Time on 100 Batches: 43 seconds\n",
            "Epoch:   3/100, Batch:  700/4120, Training Loss Error:  1.806, Training Time on 100 Batches: 49 seconds\n",
            "Epoch:   3/100, Batch:  800/4120, Training Loss Error:  1.834, Training Time on 100 Batches: 47 seconds\n",
            "Epoch:   3/100, Batch:  900/4120, Training Loss Error:  1.857, Training Time on 100 Batches: 49 seconds\n",
            "Epoch:   3/100, Batch: 1000/4120, Training Loss Error:  1.840, Training Time on 100 Batches: 48 seconds\n",
            "Epoch:   3/100, Batch: 1100/4120, Training Loss Error:  1.864, Training Time on 100 Batches: 50 seconds\n",
            "Epoch:   3/100, Batch: 1200/4120, Training Loss Error:  1.837, Training Time on 100 Batches: 48 seconds\n",
            "Epoch:   3/100, Batch: 1300/4120, Training Loss Error:  1.857, Training Time on 100 Batches: 51 seconds\n",
            "Epoch:   3/100, Batch: 1400/4120, Training Loss Error:  1.799, Training Time on 100 Batches: 51 seconds\n",
            "Epoch:   3/100, Batch: 1500/4120, Training Loss Error:  1.835, Training Time on 100 Batches: 49 seconds\n",
            "Epoch:   3/100, Batch: 1600/4120, Training Loss Error:  1.824, Training Time on 100 Batches: 47 seconds\n",
            "Epoch:   3/100, Batch: 1700/4120, Training Loss Error:  1.861, Training Time on 100 Batches: 51 seconds\n",
            "Epoch:   3/100, Batch: 1800/4120, Training Loss Error:  1.847, Training Time on 100 Batches: 44 seconds\n",
            "Epoch:   3/100, Batch: 1900/4120, Training Loss Error:  1.825, Training Time on 100 Batches: 51 seconds\n",
            "Epoch:   3/100, Batch: 2000/4120, Training Loss Error:  1.830, Training Time on 100 Batches: 52 seconds\n",
            "Validation Loss Error:  1.807, Batch Validation Time: 89 seconds\n",
            "I speak better now!!\n",
            "Epoch:   3/100, Batch: 2100/4120, Training Loss Error:  1.816, Training Time on 100 Batches: 49 seconds\n",
            "Epoch:   3/100, Batch: 2200/4120, Training Loss Error:  1.920, Training Time on 100 Batches: 48 seconds\n",
            "Epoch:   3/100, Batch: 2300/4120, Training Loss Error:  1.853, Training Time on 100 Batches: 54 seconds\n",
            "Epoch:   3/100, Batch: 2400/4120, Training Loss Error:  1.829, Training Time on 100 Batches: 53 seconds\n",
            "Epoch:   3/100, Batch: 2500/4120, Training Loss Error:  1.880, Training Time on 100 Batches: 48 seconds\n",
            "Epoch:   3/100, Batch: 2600/4120, Training Loss Error:  1.863, Training Time on 100 Batches: 50 seconds\n",
            "Epoch:   3/100, Batch: 2700/4120, Training Loss Error:  1.883, Training Time on 100 Batches: 56 seconds\n",
            "Epoch:   3/100, Batch: 2800/4120, Training Loss Error:  1.877, Training Time on 100 Batches: 54 seconds\n",
            "Epoch:   3/100, Batch: 2900/4120, Training Loss Error:  1.863, Training Time on 100 Batches: 55 seconds\n",
            "Epoch:   3/100, Batch: 3000/4120, Training Loss Error:  1.843, Training Time on 100 Batches: 51 seconds\n",
            "Epoch:   3/100, Batch: 3100/4120, Training Loss Error:  1.928, Training Time on 100 Batches: 48 seconds\n",
            "Epoch:   3/100, Batch: 3200/4120, Training Loss Error:  1.806, Training Time on 100 Batches: 52 seconds\n",
            "Epoch:   3/100, Batch: 3300/4120, Training Loss Error:  1.833, Training Time on 100 Batches: 58 seconds\n",
            "Epoch:   3/100, Batch: 3400/4120, Training Loss Error:  1.868, Training Time on 100 Batches: 59 seconds\n",
            "Epoch:   3/100, Batch: 3500/4120, Training Loss Error:  1.833, Training Time on 100 Batches: 59 seconds\n",
            "Epoch:   3/100, Batch: 3600/4120, Training Loss Error:  1.830, Training Time on 100 Batches: 59 seconds\n",
            "Epoch:   3/100, Batch: 3700/4120, Training Loss Error:  1.865, Training Time on 100 Batches: 54 seconds\n",
            "Epoch:   3/100, Batch: 3800/4120, Training Loss Error:  1.816, Training Time on 100 Batches: 66 seconds\n",
            "Epoch:   3/100, Batch: 3900/4120, Training Loss Error:  1.869, Training Time on 100 Batches: 60 seconds\n",
            "Epoch:   3/100, Batch: 4000/4120, Training Loss Error:  1.854, Training Time on 100 Batches: 63 seconds\n",
            "Epoch:   3/100, Batch: 4100/4120, Training Loss Error:  1.861, Training Time on 100 Batches: 66 seconds\n",
            "Validation Loss Error:  1.796, Batch Validation Time: 88 seconds\n",
            "I speak better now!!\n",
            "Epoch:   4/100, Batch:    0/4120, Training Loss Error:  0.358, Training Time on 100 Batches: 41 seconds\n",
            "Epoch:   4/100, Batch:  100/4120, Training Loss Error:  1.815, Training Time on 100 Batches: 48 seconds\n",
            "Epoch:   4/100, Batch:  200/4120, Training Loss Error:  1.812, Training Time on 100 Batches: 38 seconds\n",
            "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 34791061 get requests, put_count=34791080 evicted_count=10000 eviction_rate=0.00028743 and unsatisfied allocation rate=0.000293351\n",
            "Epoch:   4/100, Batch:  300/4120, Training Loss Error:  1.799, Training Time on 100 Batches: 47 seconds\n",
            "Epoch:   4/100, Batch:  400/4120, Training Loss Error:  1.756, Training Time on 100 Batches: 44 seconds\n",
            "Epoch:   4/100, Batch:  500/4120, Training Loss Error:  1.800, Training Time on 100 Batches: 47 seconds\n",
            "Epoch:   4/100, Batch:  600/4120, Training Loss Error:  1.783, Training Time on 100 Batches: 44 seconds\n",
            "Epoch:   4/100, Batch:  700/4120, Training Loss Error:  1.768, Training Time on 100 Batches: 50 seconds\n",
            "Epoch:   4/100, Batch:  800/4120, Training Loss Error:  1.793, Training Time on 100 Batches: 48 seconds\n",
            "Epoch:   4/100, Batch:  900/4120, Training Loss Error:  1.816, Training Time on 100 Batches: 49 seconds\n",
            "Epoch:   4/100, Batch: 1000/4120, Training Loss Error:  1.806, Training Time on 100 Batches: 48 seconds\n",
            "Epoch:   4/100, Batch: 1100/4120, Training Loss Error:  1.826, Training Time on 100 Batches: 51 seconds\n",
            "Epoch:   4/100, Batch: 1200/4120, Training Loss Error:  1.798, Training Time on 100 Batches: 48 seconds\n",
            "Epoch:   4/100, Batch: 1300/4120, Training Loss Error:  1.817, Training Time on 100 Batches: 50 seconds\n",
            "Epoch:   4/100, Batch: 1400/4120, Training Loss Error:  1.765, Training Time on 100 Batches: 52 seconds\n",
            "Epoch:   4/100, Batch: 1500/4120, Training Loss Error:  1.800, Training Time on 100 Batches: 49 seconds\n",
            "Epoch:   4/100, Batch: 1600/4120, Training Loss Error:  1.790, Training Time on 100 Batches: 48 seconds\n",
            "Epoch:   4/100, Batch: 1700/4120, Training Loss Error:  1.825, Training Time on 100 Batches: 51 seconds\n",
            "Epoch:   4/100, Batch: 1800/4120, Training Loss Error:  1.812, Training Time on 100 Batches: 44 seconds\n",
            "Epoch:   4/100, Batch: 1900/4120, Training Loss Error:  1.789, Training Time on 100 Batches: 51 seconds\n",
            "Epoch:   4/100, Batch: 2000/4120, Training Loss Error:  1.795, Training Time on 100 Batches: 53 seconds\n",
            "Validation Loss Error:  1.781, Batch Validation Time: 89 seconds\n",
            "I speak better now!!\n",
            "Epoch:   4/100, Batch: 2100/4120, Training Loss Error:  1.782, Training Time on 100 Batches: 49 seconds\n",
            "Epoch:   4/100, Batch: 2200/4120, Training Loss Error:  1.884, Training Time on 100 Batches: 48 seconds\n",
            "Epoch:   4/100, Batch: 2300/4120, Training Loss Error:  1.818, Training Time on 100 Batches: 55 seconds\n",
            "Epoch:   4/100, Batch: 2400/4120, Training Loss Error:  1.798, Training Time on 100 Batches: 54 seconds\n",
            "Epoch:   4/100, Batch: 2500/4120, Training Loss Error:  1.843, Training Time on 100 Batches: 49 seconds\n",
            "Epoch:   4/100, Batch: 2600/4120, Training Loss Error:  1.830, Training Time on 100 Batches: 51 seconds\n",
            "Epoch:   4/100, Batch: 2700/4120, Training Loss Error:  1.852, Training Time on 100 Batches: 57 seconds\n",
            "Epoch:   4/100, Batch: 2800/4120, Training Loss Error:  1.841, Training Time on 100 Batches: 54 seconds\n",
            "Epoch:   4/100, Batch: 2900/4120, Training Loss Error:  1.830, Training Time on 100 Batches: 56 seconds\n",
            "Epoch:   4/100, Batch: 3000/4120, Training Loss Error:  1.811, Training Time on 100 Batches: 53 seconds\n",
            "Epoch:   4/100, Batch: 3100/4120, Training Loss Error:  1.895, Training Time on 100 Batches: 49 seconds\n",
            "Epoch:   4/100, Batch: 3200/4120, Training Loss Error:  1.775, Training Time on 100 Batches: 53 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}