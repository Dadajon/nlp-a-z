# Additional Resources

The supplementary materials are below. Enjoy!

[SuperDataScience Team](https://www.superdatascience.com/pages/deep-learning-chatbot)

### So, What is NLP?
NLP is an interdisciplinary field concerned with the interactions between computers and human natural languages (e.g: English) — speech or text. NLP-powered softwares help us in our daily lives in various ways, for example:

- **Personal assistants**: Siri, Cortana, and Google Assistant.
- **Auto-complete**: In search engines (e.g: Google, Bing).
- **Spell checking**: Almost everywhere, in your browser, your IDE (e.g: Visual Studio), desktop apps (e.g: Microsoft Word).
- **Machine Translation**: Google Translate.

Okay, now we get it, NLP plays a major role in our daily computer interactions, let’s see more business-related NLP use-cases:

- If you have a bank or a restaurant with a huge load of customers orders and complaints, handling this in a manual way will be tiresome and repetitive, also not very efficient in terms of time and labor, so you can build a **chat-bot** for your business, which will automate such process and reduce human interaction.

- Apple launched the new iPhone 11, and they are interested to know what users are thinking of the new iPhone, so they can monitor social media channels (e.g: Twitter), and extract iPhone 11 related tweets, reviews, and opinions, then use **sentiment analysis** models to predict whether users’ reviews are positive, negative, or neutral.

NLP is divided into two fields: Linguistics and Computer Science.

![image](https://miro.medium.com/max/1430/1*mOYZab1OONz33eICVc8QFA.jpeg)

The **Linguistics** side is concerned with language, it’s formation, syntax, meaning, different kind of phrases (noun or verb) and whatnot.

The **Computer Science** side is concerned with applying linguistic knowledge, by transforming it into computer programs with the help of sub-fields such as Artificial Intelligence (Machine Learning & Deep Learning).

### Let’s Talk Science!

Scientific advancements in NLP can be divided into 3 categories (Rule-based systems, Classical Machine Learning models and Deep Learning models).

- **Rule-based systems** rely heavily on crafting domain-specific rules (e.g: regular expressions), can be used to solve simple problems such as extracting structured data (e.g: emails) from unstructured data (e.g: web-pages), but due to the complexity of human natural languages, rule-based systems fail to build models that can really reason about language.

- **Classical Machine Learning** approaches can be used to solve harder problems which rule-based systems can’t solve very well (e.g: Spam Detection), it rely on a more general approach to understanding language, using hand-crafted features (e.g: sentence length, part of speech tags, occurrence of specific words) then providing those features to a statistical machine learning model (e.g: Naive Bayes), which learns different patterns in the training set and then be able to reason about unseen data (inference).

- **Deep Learning models** are the hottest part of NLP research and applications now, they generalize even better than the classical machine learning approaches as they don’t need hand-crafted features because they work as feature extractors in an automatic way, which helped a lot in building end-to-end models (little human-interaction). Aside from the feature engineering part, deep learning algorithms learning capabilities are more powerful than the shallow/classical ML ones, which paved its way to achieving the highest scores on different hard NLP tasks (e.g: Machine Translation).

![itsalive](https://miro.medium.com/max/2649/1*E5Dsa1lBCYBdVD_hdRiTJw.png)